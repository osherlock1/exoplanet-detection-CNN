{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMU7PqvYgQiuUi88ADRzqa5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/osherlock1/exoplanet-detection-CNN/blob/main/exoplanet_cnn_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exoplanet Detection CNN Modle\n",
        "\n",
        "**Project Overview:**\n",
        "This notebook implemetns a complete pipeline for automatic exoplanet transit destection in stellar light curve data from the Kepler-2 Misison.  The light curves are classified using a lightweight 1D CNN.\n",
        "\n"
      ],
      "metadata": {
        "id": "FguOClXiuS9V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "GsNDG79Lud_f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ovlzp-GPtqwS"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, TensorDataset\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import random\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.metrics import roc_curve, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim.lr_scheduler import StepLR"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Connect to Google Drive**"
      ],
      "metadata": {
        "id": "LVF_qv__u0xt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "kKvY2iNAuiFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Config"
      ],
      "metadata": {
        "id": "2t9BhOMf8HeT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CONFIRMED_DATA_DIR   = '/content/drive/MyDrive/ELE391_Final_Project/data_v4/K2_conf_pt'\n",
        "FALSE_POS_DIR = '/content/drive/MyDrive/ELE391_Final_Project/data_v4/K2_fp_pt'\n",
        "BATCH_SIZE = 64\n",
        "LR         = 1e-3\n",
        "EPOCHS     = 20\n",
        "DEVICE     = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)"
      ],
      "metadata": {
        "id": "X6IspICy8JUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Processed Lightcurve Data"
      ],
      "metadata": {
        "id": "qDAT22RMxK0D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Path to processed data saved in my drive\n",
        "confirmed_planets_dir = CONFIRMED_DATA_DIR\n",
        "false_positives_dir = FALSE_POS_DIR\n",
        "\n",
        "# Helper function to load tensors and add labels\n",
        "def load_and_label_tensors(directory, label):\n",
        "    tensors = []\n",
        "    labels = []\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith('.pt'):\n",
        "            filepath = os.path.join(directory, filename)\n",
        "            lightcurve = torch.load(filepath)\n",
        "            tensors.append(lightcurve)  # Add batch dimension\n",
        "            labels.append(label)  # Assign the label\n",
        "    return tensors, labels\n",
        "\n",
        "# Load confirmed planets (label 1)\n",
        "confirmed_tensors, confirmed_labels = load_and_label_tensors(confirmed_planets_dir, label=1)\n",
        "\n",
        "# Load false positives (label 0)\n",
        "false_tensors, false_labels = load_and_label_tensors(false_positives_dir, label=0)\n",
        "\n",
        "# Combine the tensors and labels\n",
        "all_tensors = confirmed_tensors + false_tensors\n",
        "all_labels = confirmed_labels + false_labels"
      ],
      "metadata": {
        "id": "GGoLiS7NxP2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stack all light curves into a single tensor (num_samples, 200)\n",
        "data = torch.stack(all_tensors)\n",
        "\n",
        "# Convert labels to a tensor (num_samples,)\n",
        "labels = torch.tensor(all_labels, dtype=torch.long)\n",
        "\n",
        "print(\"Data Shape:\", data.shape)\n",
        "print(\"Labels Shape:\", labels.shape)"
      ],
      "metadata": {
        "id": "otA7iekzxbw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Augmentatino with SMOTE**"
      ],
      "metadata": {
        "id": "nX0m5xcqyWMv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "smote = SMOTE(random_state=42)  # You can set a random state for reproducibility\n",
        "balanced_data_np, balanced_labels_np = smote.fit_resample(data.cpu().numpy(), labels.cpu().numpy())\n",
        "\n",
        "# Convert back to PyTorch tensors\n",
        "data = torch.tensor(balanced_data_np, dtype=data.dtype, device=data.device)\n",
        "labels = torch.tensor(balanced_labels_np, dtype=labels.dtype, device=labels.device)\n",
        "data = data.unsqueeze(1)"
      ],
      "metadata": {
        "id": "MgZApr7UyYH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Normalize Dataset**"
      ],
      "metadata": {
        "id": "RBK_5jsJylju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize each sample in the dataset\n",
        "data_mean = data.mean(dim=-1, keepdim=True)  # Mean across the sequence\n",
        "data_std = data.std(dim=-1, keepdim=True)    # Standard deviation across the sequence\n",
        "data = (data - data_mean) / data_std\n",
        "\n",
        "dataset = TensorDataset(data, labels)"
      ],
      "metadata": {
        "id": "hpXEUhUuynnf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Split the Dataset**"
      ],
      "metadata": {
        "id": "woi5u4jJyuu6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_size = len(dataset)\n",
        "train_size = int(0.8* dataset_size)\n",
        "val_size = int(0.1 * dataset_size)\n",
        "test_size = dataset_size - train_size - val_size\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])"
      ],
      "metadata": {
        "id": "nkFCh9t_ywWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create DataLoaders**"
      ],
      "metadata": {
        "id": "bw7LnnzHy19V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "batch_size = 16\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size, shuffle = True, drop_last = True)\n",
        "val_loader = DataLoader(val_dataset, batch_size, shuffle = True, drop_last = True)\n",
        "test_loader = DataLoader(test_dataset, batch_size, shuffle = False, drop_last = True)"
      ],
      "metadata": {
        "id": "WnoHtG1vy3aW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNN Model"
      ],
      "metadata": {
        "id": "LofkvktGzESp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvolutionalNetwork(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(ConvolutionalNetwork, self).__init__()\n",
        "    #Describe convolutional layer and what it's doing (2 convolutional layers)\n",
        "    self.conv1 = nn.Conv1d(in_channels = 1, out_channels = 16, kernel_size = 4, stride = 2, padding = 1)\n",
        "    self.conv2 = nn.Conv1d(in_channels = 16, out_channels = 32, kernel_size = 8, stride = 2, padding = 1)\n",
        "    self.conv3 = nn.Conv1d(in_channels = 32, out_channels =  64, kernel_size = 12, stride = 2, padding = 1)\n",
        "    self.conv4 = nn.Conv1d(in_channels = 64, out_channels = 16, kernel_size = 20, stride = 2, padding = 1)\n",
        "    self.conv5 = nn.Conv1d(in_channels = 16, out_channels = 16, kernel_size = 4, stride = 2, padding = 1)\n",
        "\n",
        "    dummy_input = torch.randn(1, 1, 400)\n",
        "    x = F.relu(self.conv1(dummy_input))\n",
        "    x = F.relu(self.conv2(x))\n",
        "    x = F.relu(self.conv3(x))\n",
        "    x = F.relu(self.conv4(x))\n",
        "    x = F.relu(self.conv5(x))\n",
        "    flattened_size = x.view(x.size(0), -1).size(1)\n",
        "\n",
        "\n",
        "    self.fc1 = nn.Linear(flattened_size, 128)\n",
        "    self.fc2 = nn.Linear(128,128)\n",
        "    self.fc3 = nn.Linear(128,2)\n",
        "\n",
        "  def forward(self, x):\n",
        "    X = F.relu(self.conv1(x))\n",
        "    X = F.relu(self.conv2(X))\n",
        "    X = F.relu(self.conv3(X))\n",
        "    X = F.relu(self.conv4(X))\n",
        "    X = F.relu(self.conv5(X))\n",
        "\n",
        "\n",
        "    X = X.view(X.size(0), -1)\n",
        "\n",
        "    #Fully Connected Layers\n",
        "    X = F.relu(self.fc1(X))\n",
        "    X = F.relu(self.fc2(X))\n",
        "    X = self.fc3(X)\n",
        "\n",
        "    return F.log_softmax(X, dim = 1)\n",
        "\n",
        "model = ConvolutionalNetwork().to(DEVICE)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR)"
      ],
      "metadata": {
        "id": "ym89Q6bqzGDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Loop"
      ],
      "metadata": {
        "id": "11oBjkJSzWMj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "train_correct= []\n",
        "val_correct =[]\n",
        "\n",
        "scheduler = StepLR(optimizer, step_size=10, gamma=0.5)\n",
        "\n",
        "\n",
        "for i in range(EPOCHS):\n",
        "  trn_corr = 0\n",
        "  val_corr = 0\n",
        "\n",
        "  for data, labels in train_loader:\n",
        "\n",
        "    y_pred = model(data)\n",
        "    loss = criterion(y_pred, labels)\n",
        "\n",
        "    predicted = torch.max(y_pred.data, 1)[1]\n",
        "    batch_corr = (predicted == labels).sum()\n",
        "    trn_corr += batch_corr # keep track as we go along\n",
        "\n",
        "    #Update our paremeters\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  train_losses.append(loss)\n",
        "  train_correct.append(trn_corr)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for data, labels in val_loader:\n",
        "      y_val = model(data)\n",
        "      predicted = torch.max(y_val.data, 1)[1]\n",
        "      val_corr += (predicted == labels).sum()\n",
        "\n",
        "    val_loss = criterion(y_val, labels)\n",
        "  val_losses.append(val_loss.item())\n",
        "  val_correct.append(val_corr.item())\n",
        "  scheduler.step()\n",
        "  train_acc = trn_corr.item() / len(train_loader.dataset) * 100\n",
        "  val_acc = val_corr.item() / len(val_loader.dataset) * 100\n",
        "\n",
        "  print(f\"Epoch {i+1}/{EPOCHS}\")\n",
        "  print(f\"Train Loss: {loss.item():.4f} | Train Accuracy: {train_acc:.2f}%\")\n",
        "  print(f\"Val Loss: {val_loss.item():.4f} | Val Accuracy: {val_acc:.2f}%\")"
      ],
      "metadata": {
        "id": "MknIYkoVzZB8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate"
      ],
      "metadata": {
        "id": "Azcj_irpz222"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_corr = 0\n",
        "test_loss = 0\n",
        "all_labels = []\n",
        "all_predictions = []\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():  # Disable gradient computation\n",
        "    for data, labels in test_loader:\n",
        "        y_test = model(data)  # Forward pass\n",
        "        test_loss += criterion(y_test, labels).item()  # Accumulate test loss\n",
        "        predicted = torch.max(y_test.data, 1)[1]  # Get predictions\n",
        "        test_corr += (predicted == labels).sum()  # Count correct predictions\n",
        "\n",
        "        # Store predictions and labels for metrics\n",
        "        all_predictions.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Calculate test accuracy and average loss\n",
        "test_acc = test_corr.item() / len(test_loader.dataset) * 100\n",
        "test_loss /= len(test_loader)\n",
        "\n",
        "# Calculate precision, recall, and F1-score\n",
        "precision = precision_score(all_labels, all_predictions, average=\"weighted\")\n",
        "recall = recall_score(all_labels, all_predictions, average=\"weighted\")\n",
        "f1 = f1_score(all_labels, all_predictions, average=\"weighted\")\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f} | Test Accuracy: {test_acc:.2f}%\")\n",
        "print(f\"Precision: {precision:.2f} | Recall: {recall:.2f} | F1-Score: {f1:.2f}\")"
      ],
      "metadata": {
        "id": "vI9-GGCXzo10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model.to(DEVICE)\n",
        "model.eval()\n",
        "\n",
        "\n",
        "y_true, y_pred, y_proba = [], [], []\n",
        "with torch.no_grad():\n",
        "    for x, y in val_loader:                         # or test_loader\n",
        "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "        logits = model(x)\n",
        "        probs = F.softmax(logits, dim=1)[:, 1]       # P(transit)\n",
        "        preds = logits.argmax(dim=1)\n",
        "        y_true.extend(y.cpu().numpy())\n",
        "        y_pred.extend(preds.cpu().numpy())\n",
        "        y_proba.extend(probs.cpu().numpy())\n",
        "\n",
        "#Classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(\n",
        "    y_true,\n",
        "    y_pred,\n",
        "    target_names=[\"No Transit\", \"Transit\"],\n",
        "))\n",
        "\n",
        "#Confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
        "                              display_labels=[\"No\", \"Transit\"])\n",
        "disp.plot()\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "u7H2WWT3_lJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4OdGN6JN_nOl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}