{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Exoplanet Detection CNN Modle\n",
        "\n"
      ],
      "metadata": {
        "id": "FguOClXiuS9V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "GsNDG79Lud_f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Ovlzp-GPtqwS"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "#import mlflow\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, TensorDataset\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "#import mlflow\n",
        "#import mlflow.pytorch\n",
        "from torch.optim.lr_scheduler import StepLR"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Connect to Google Drive**"
      ],
      "metadata": {
        "id": "LVF_qv__u0xt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKvY2iNAuiFu",
        "outputId": "4c07814b-cf96-4b81-df06-23d6fe16b732"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Processed Lightcurve Data"
      ],
      "metadata": {
        "id": "qDAT22RMxK0D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Path to processed data saved in my drive\n",
        "confirmed_planets_dir = '/content/drive/MyDrive/ELE391_Final_Project/data_v4/K2_conf_pt'\n",
        "false_positives_dir = '/content/drive/MyDrive/ELE391_Final_Project/data_v4/K2_fp_pt'\n",
        "\n",
        "# Helper function to load tensors and add labels\n",
        "def load_and_label_tensors(directory, label):\n",
        "    tensors = []\n",
        "    labels = []\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith('.pt'):\n",
        "            filepath = os.path.join(directory, filename)\n",
        "            lightcurve = torch.load(filepath)\n",
        "            tensors.append(lightcurve)  # Add batch dimension\n",
        "            labels.append(label)  # Assign the label\n",
        "    return tensors, labels\n",
        "\n",
        "# Load confirmed planets (label 1)\n",
        "confirmed_tensors, confirmed_labels = load_and_label_tensors(confirmed_planets_dir, label=1)\n",
        "\n",
        "# Load false positives (label 0)\n",
        "false_tensors, false_labels = load_and_label_tensors(false_positives_dir, label=0)\n",
        "\n",
        "# Combine the tensors and labels\n",
        "all_tensors = confirmed_tensors + false_tensors\n",
        "all_labels = confirmed_labels + false_labels"
      ],
      "metadata": {
        "id": "GGoLiS7NxP2b"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stack all light curves into a single tensor (num_samples, 200)\n",
        "data = torch.stack(all_tensors)\n",
        "\n",
        "# Convert labels to a tensor (num_samples,)\n",
        "labels = torch.tensor(all_labels, dtype=torch.long)\n",
        "\n",
        "print(\"Data Shape:\", data.shape)\n",
        "print(\"Labels Shape:\", labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otA7iekzxbw5",
        "outputId": "64df6b27-374b-40b8-ce21-51007048c3dc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Shape: torch.Size([1002, 400])\n",
            "Labels Shape: torch.Size([1002])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Augmentatino with SMOTE**"
      ],
      "metadata": {
        "id": "nX0m5xcqyWMv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "smote = SMOTE(random_state=42)  # You can set a random state for reproducibility\n",
        "balanced_data_np, balanced_labels_np = smote.fit_resample(data.cpu().numpy(), labels.cpu().numpy())\n",
        "\n",
        "# Convert back to PyTorch tensors\n",
        "data = torch.tensor(balanced_data_np, dtype=data.dtype, device=data.device)\n",
        "labels = torch.tensor(balanced_labels_np, dtype=labels.dtype, device=labels.device)\n",
        "data = data.unsqueeze(1)"
      ],
      "metadata": {
        "id": "MgZApr7UyYH-"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Normalize Dataset**"
      ],
      "metadata": {
        "id": "RBK_5jsJylju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize each sample in the dataset\n",
        "data_mean = data.mean(dim=-1, keepdim=True)  # Mean across the sequence\n",
        "data_std = data.std(dim=-1, keepdim=True)    # Standard deviation across the sequence\n",
        "data = (data - data_mean) / data_std\n",
        "\n",
        "dataset = TensorDataset(data, labels)"
      ],
      "metadata": {
        "id": "hpXEUhUuynnf"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Split the Dataset**"
      ],
      "metadata": {
        "id": "woi5u4jJyuu6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_size = len(dataset)\n",
        "train_size = int(0.8* dataset_size)\n",
        "val_size = int(0.1 * dataset_size)\n",
        "test_size = dataset_size - train_size - val_size\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])"
      ],
      "metadata": {
        "id": "nkFCh9t_ywWa"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create DataLoaders**"
      ],
      "metadata": {
        "id": "bw7LnnzHy19V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "batch_size = 16\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size, shuffle = True, drop_last = True)\n",
        "val_loader = DataLoader(val_dataset, batch_size, shuffle = True, drop_last = True)\n",
        "test_loader = DataLoader(test_dataset, batch_size, shuffle = False, drop_last = True)"
      ],
      "metadata": {
        "id": "WnoHtG1vy3aW"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNN Model"
      ],
      "metadata": {
        "id": "LofkvktGzESp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvolutionalNetwork(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(ConvolutionalNetwork, self).__init__()\n",
        "    #Describe convolutional layer and what it's doing (2 convolutional layers)\n",
        "    self.conv1 = nn.Conv1d(in_channels = 1, out_channels = 16, kernel_size = 4, stride = 2, padding = 1)\n",
        "    self.conv2 = nn.Conv1d(in_channels = 16, out_channels = 32, kernel_size = 8, stride = 2, padding = 1)\n",
        "    self.conv3 = nn.Conv1d(in_channels = 32, out_channels =  64, kernel_size = 12, stride = 2, padding = 1)\n",
        "    self.conv4 = nn.Conv1d(in_channels = 64, out_channels = 16, kernel_size = 20, stride = 2, padding = 1)\n",
        "    self.conv5 = nn.Conv1d(in_channels = 16, out_channels = 16, kernel_size = 4, stride = 2, padding = 1)\n",
        "\n",
        "\n",
        "\n",
        "    self.fc1 = nn.Linear(16 * 7,128)\n",
        "    self.fc2 = nn.Linear(128,128)\n",
        "    self.fc3 = nn.Linear(128,2)\n",
        "\n",
        "  def forward(self, x):\n",
        "    X = F.relu(self.conv1(x))\n",
        "    X = F.relu(self.conv2(X))\n",
        "    X = F.relu(self.conv3(X))\n",
        "    X = F.relu(self.conv4(X))\n",
        "    X = F.relu(self.conv5(X))\n",
        "\n",
        "\n",
        "\n",
        "    X = X.view(-1, 16 * 7)\n",
        "\n",
        "    #Fully Connected Layers\n",
        "    X = F.relu(self.fc1(X))\n",
        "    X = F.relu(self.fc2(X))\n",
        "    X = self.fc3(X)\n",
        "\n",
        "    return F.log_softmax(X, dim = 1)\n"
      ],
      "metadata": {
        "id": "ym89Q6bqzGDz"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create instance of the model\n",
        "torch.manual_seed(41)\n",
        "model = ConvolutionalNetwork()"
      ],
      "metadata": {
        "id": "2AjBrN0ZzLho"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss Function Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)"
      ],
      "metadata": {
        "id": "Q7g9kmJ3zRzO"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Loop"
      ],
      "metadata": {
        "id": "11oBjkJSzWMj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "#For loops of Epochs\n",
        "epochs = 20\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "train_correct= []\n",
        "val_correct =[]\n",
        "\n",
        "scheduler = StepLR(optimizer, step_size=10, gamma=0.5)\n",
        "\n",
        "\n",
        "for i in range(epochs):\n",
        "  trn_corr = 0\n",
        "  val_corr = 0\n",
        "\n",
        "  for data, labels in train_loader:\n",
        "\n",
        "    y_pred = model(data)\n",
        "    loss = criterion(y_pred, labels)\n",
        "\n",
        "    predicted = torch.max(y_pred.data, 1)[1]\n",
        "    batch_corr = (predicted == labels).sum()\n",
        "    trn_corr += batch_corr # keep track as we go along\n",
        "\n",
        "    #Update our paremeters\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  train_losses.append(loss)\n",
        "  train_correct.append(trn_corr)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for data, labels in val_loader:\n",
        "      y_val = model(data)\n",
        "      predicted = torch.max(y_val.data, 1)[1]\n",
        "      val_corr += (predicted == labels).sum()\n",
        "\n",
        "    val_loss = criterion(y_val, labels)\n",
        "  val_losses.append(val_loss.item())\n",
        "  val_correct.append(val_corr.item())\n",
        "  scheduler.step()\n",
        "  train_acc = trn_corr.item() / len(train_loader.dataset) * 100\n",
        "  val_acc = val_corr.item() / len(val_loader.dataset) * 100\n",
        "\n",
        "  print(f\"Epoch {i+1}/{epochs}\")\n",
        "  print(f\"Train Loss: {loss.item():.4f} | Train Accuracy: {train_acc:.2f}%\")\n",
        "  print(f\"Val Loss: {val_loss.item():.4f} | Val Accuracy: {val_acc:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MknIYkoVzZB8",
        "outputId": "5f37b58b-47b3-4ebd-e0fc-cfbbcba64a2a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "Train Loss: 0.6927 | Train Accuracy: 50.94%\n",
            "Val Loss: 0.7233 | Val Accuracy: 51.25%\n",
            "Epoch 2/20\n",
            "Train Loss: 0.7310 | Train Accuracy: 54.92%\n",
            "Val Loss: 0.7124 | Val Accuracy: 62.50%\n",
            "Epoch 3/20\n",
            "Train Loss: 0.4404 | Train Accuracy: 73.28%\n",
            "Val Loss: 0.6171 | Val Accuracy: 74.38%\n",
            "Epoch 4/20\n",
            "Train Loss: 0.5276 | Train Accuracy: 82.27%\n",
            "Val Loss: 0.3931 | Val Accuracy: 76.25%\n",
            "Epoch 5/20\n",
            "Train Loss: 0.2767 | Train Accuracy: 88.75%\n",
            "Val Loss: 0.1690 | Val Accuracy: 83.12%\n",
            "Epoch 6/20\n",
            "Train Loss: 0.1744 | Train Accuracy: 92.03%\n",
            "Val Loss: 0.6338 | Val Accuracy: 78.12%\n",
            "Epoch 7/20\n",
            "Train Loss: 0.1625 | Train Accuracy: 95.00%\n",
            "Val Loss: 0.5569 | Val Accuracy: 88.75%\n",
            "Epoch 8/20\n",
            "Train Loss: 0.0187 | Train Accuracy: 97.27%\n",
            "Val Loss: 0.6408 | Val Accuracy: 86.88%\n",
            "Epoch 9/20\n",
            "Train Loss: 0.0054 | Train Accuracy: 97.73%\n",
            "Val Loss: 0.3652 | Val Accuracy: 88.12%\n",
            "Epoch 10/20\n",
            "Train Loss: 0.0154 | Train Accuracy: 99.14%\n",
            "Val Loss: 1.1049 | Val Accuracy: 88.75%\n",
            "Epoch 11/20\n",
            "Train Loss: 0.0031 | Train Accuracy: 99.22%\n",
            "Val Loss: 0.3928 | Val Accuracy: 90.00%\n",
            "Epoch 12/20\n",
            "Train Loss: 0.0140 | Train Accuracy: 99.69%\n",
            "Val Loss: 0.9021 | Val Accuracy: 92.50%\n",
            "Epoch 13/20\n",
            "Train Loss: 0.0026 | Train Accuracy: 99.69%\n",
            "Val Loss: 0.9142 | Val Accuracy: 86.88%\n",
            "Epoch 14/20\n",
            "Train Loss: 0.0001 | Train Accuracy: 99.92%\n",
            "Val Loss: 0.5487 | Val Accuracy: 90.00%\n",
            "Epoch 15/20\n",
            "Train Loss: 0.0002 | Train Accuracy: 100.00%\n",
            "Val Loss: 1.1530 | Val Accuracy: 90.00%\n",
            "Epoch 16/20\n",
            "Train Loss: 0.0001 | Train Accuracy: 100.00%\n",
            "Val Loss: 0.3991 | Val Accuracy: 90.62%\n",
            "Epoch 17/20\n",
            "Train Loss: 0.0001 | Train Accuracy: 100.00%\n",
            "Val Loss: 1.3442 | Val Accuracy: 90.62%\n",
            "Epoch 18/20\n",
            "Train Loss: 0.0002 | Train Accuracy: 100.00%\n",
            "Val Loss: 2.4845 | Val Accuracy: 90.62%\n",
            "Epoch 19/20\n",
            "Train Loss: 0.0004 | Train Accuracy: 100.00%\n",
            "Val Loss: 0.6189 | Val Accuracy: 90.62%\n",
            "Epoch 20/20\n",
            "Train Loss: 0.0001 | Train Accuracy: 100.00%\n",
            "Val Loss: 0.9498 | Val Accuracy: 90.62%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate"
      ],
      "metadata": {
        "id": "Azcj_irpz222"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_corr = 0\n",
        "test_loss = 0\n",
        "all_labels = []\n",
        "all_predictions = []\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():  # Disable gradient computation\n",
        "    for data, labels in test_loader:\n",
        "        y_test = model(data)  # Forward pass\n",
        "        test_loss += criterion(y_test, labels).item()  # Accumulate test loss\n",
        "        predicted = torch.max(y_test.data, 1)[1]  # Get predictions\n",
        "        test_corr += (predicted == labels).sum()  # Count correct predictions\n",
        "\n",
        "        # Store predictions and labels for metrics\n",
        "        all_predictions.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Calculate test accuracy and average loss\n",
        "test_acc = test_corr.item() / len(test_loader.dataset) * 100\n",
        "test_loss /= len(test_loader)\n",
        "\n",
        "# Calculate precision, recall, and F1-score\n",
        "precision = precision_score(all_labels, all_predictions, average=\"weighted\")\n",
        "recall = recall_score(all_labels, all_predictions, average=\"weighted\")\n",
        "f1 = f1_score(all_labels, all_predictions, average=\"weighted\")\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f} | Test Accuracy: {test_acc:.2f}%\")\n",
        "print(f\"Precision: {precision:.2f} | Recall: {recall:.2f} | F1-Score: {f1:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vI9-GGCXzo10",
        "outputId": "2fb1f086-2975-44da-ee1b-ec2ae0456011"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.3161 | Test Accuracy: 95.00%\n",
            "Precision: 0.95 | Recall: 0.95 | F1-Score: 0.95\n"
          ]
        }
      ]
    }
  ]
}